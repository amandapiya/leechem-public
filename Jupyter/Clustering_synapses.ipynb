{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from numpy import sin, cos, pi\n",
    "import pandas as pd\n",
    "import plotly.offline\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import Mesh3d\n",
    "\n",
    "import nn_clustering\n",
    "\n",
    "from sbemdb import SBEMDB\n",
    "from cleandb import clean_db_uct\n",
    "from get_tree_ids import get_tree_ids\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting and preparing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees Before 398\n",
      "trees After 46\n",
      "nodes Before 37481\n",
      "nodes After 16320\n",
      "nodecons Before 73994\n",
      "nodecons After 32548\n",
      "syncons Before 1199\n",
      "syncons After 826\n",
      "synapses Before 552\n",
      "synapses After 535\n"
     ]
    }
   ],
   "source": [
    "db = SBEMDB()\n",
    "db = clean_db_uct(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From database get IDs of all trees except tree 444."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tree_ids = get_tree_ids(db,'tid != 444')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the Synapses from each tree, that have at least 2 connection to the tree 444."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trees = {}\n",
    "all_synapses = []\n",
    "syn_tid = {}\n",
    "\n",
    "for tree_id in all_tree_ids:\n",
    "    _x, _y, _z, _, _, _sid, _pre_nid, _post_nid = db.synapses(\n",
    "        f'pre.tid={tree_id} and post.tid=444', extended=True)\n",
    "\n",
    "    all_synapses_from_free = {}\n",
    "    for i in range(len(_sid)):\n",
    "        x = _x[i]\n",
    "        y = _y[i]\n",
    "        z = _z[i]\n",
    "        sid = _sid[i]\n",
    "        pre_nid = _pre_nid[i]\n",
    "        post_nid = _post_nid[i]\n",
    "        \n",
    "        syn_tid[sid] = tree_id\n",
    "\n",
    "        # Overwrites sid duplicates. Only the last one returned from\n",
    "        # that method remains (and exact coords for synapses with different pre- and postnodes should not matter).\n",
    "        all_synapses_from_free[sid] = nn_clustering.Synapse(\n",
    "            sid, x, y, z, pre_nid, post_nid\n",
    "        )\n",
    "\n",
    "    # Condition based on synapse count on tree.\n",
    "    if len(all_synapses_from_free) >= 2:\n",
    "        all_trees[tree_id] = list(all_synapses_from_free.values())\n",
    "        all_synapses += list(all_synapses_from_free.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree_id in all_trees:\n",
    "\n",
    "    tree_synapses = all_trees[tree_id]\n",
    "    #print(f\"From tree: {tree_id}:\\nSynapses:\\n{tree_synapses}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the data available analysis can be done. For it we need some set up.\n",
    "Define parameters, select functions and instantiate objectets needed for the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(param_1, param_2):\n",
    "\n",
    "    # Create the distance calculator object.\n",
    "    path_matrix = os.path.join('..','data', 'distance_matrix_path.npy') \n",
    "    matrix = np.load(path_matrix)\n",
    "    #matrix = np.load('distance_matrix_path.npy')\n",
    "    distance_calculator = nn_clustering.MatrixSynapseDistance(matrix)\n",
    "\n",
    "    # Constraint classes define how the clusters (in this case their geometry) is limited in\n",
    "    # the hierarchical clustering algorithm.\n",
    "    constraint_chaining = nn_clustering.ConstraintChaining(distance_calculator, param_1)\n",
    "    constraint_diameter = nn_clustering.ConstraintDiameter(distance_calculator, param_2)\n",
    "\n",
    "    #   To add, or remove the \"cluster\" limits for termination condition\n",
    "    #   just add or remove values from list passed to Validator instantiation.\n",
    "    constraint_validator = nn_clustering.Validator(\n",
    "        [\n",
    "            constraint_chaining,\n",
    "            constraint_diameter,\n",
    "        ]\n",
    "    )\n",
    "    # Closest linkage --> can take any distance function with appropriate signature.\n",
    "    linkage = nn_clustering.ClosestLinkage(distance_calculator)\n",
    "    \n",
    "    clusters = nn_clustering.hierarchical_clustering(\n",
    "        linkage, constraint_validator, all_synapses)\n",
    "    \n",
    "    clustered = []\n",
    "    unclustered = []\n",
    "    for clus in clusters:\n",
    "        if len(clus) == 1:\n",
    "            unclustered += clus\n",
    "        else:\n",
    "            clustered.append(clus)\n",
    "            \n",
    "    clusters_tree_ids = []\n",
    "    for clust in clustered:\n",
    "        tree_ids_ = []\n",
    "        for syn in clust:\n",
    "            tree_ids_.append(syn_tid[syn.sid])\n",
    "        clusters_tree_ids.append(tree_ids_)\n",
    "        \n",
    "    is_homo = []\n",
    "    for clust in clusters_tree_ids:\n",
    "        is_clust_homo = np.all(clust[0] == np.array(clust))\n",
    "        is_homo.append(is_clust_homo)\n",
    "    is_homo = np.array(is_homo)\n",
    "    hetero_frac = (is_homo == False).sum()/len(is_homo)\n",
    "                \n",
    "    return clustered, unclustered, hetero_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_tid = {}\n",
    "xx, yy, zz, pretid, posttid, synid, prenid, postnid = db.synapses(extended=True)\n",
    "for i in range(len(xx)):\n",
    "    sid_tid[synid[i]] = pretid[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = np.arange(2.5, 30, 2.5)\n",
    "params_2 = np.arange(10, 125, 5)\n",
    "\n",
    "\n",
    "num_clusters = []\n",
    "fractions = []\n",
    "syn_stats = pd.DataFrame(columns=['id_param_pair', 'id_cluster', 'param1', 'param2', \n",
    "                                  'total_clusters', 'num_synapses', 'hetero_fracion', 'synapse ids', 'tids'])\n",
    "id_param_pair = 0\n",
    "id_cluster = 0\n",
    "\n",
    "data_to_save = {}\n",
    "\n",
    "for p1 in params_1:\n",
    "    n_cl_ = []\n",
    "    frac_ = []\n",
    "    for p2 in params_2:\n",
    "        clusters_, unclustered_, hetero_fraction_ = find_clusters(p1, p2)\n",
    "        data_to_save[(p1, p2)] = {'clusters': clusters_, 'unclustered': unclustered_, 'hetero_fraction': hetero_fraction_}\n",
    "        n_cl_.append(len(clusters_))\n",
    "        frac_.append(hetero_fraction_)\n",
    "        for clus_ in clusters_:\n",
    "            syn_ids = [syn.sid for syn in clus_]\n",
    "            tids = [sid_tid[syn.sid] for syn in clus_]\n",
    "            syn_stats.loc[len(syn_stats)] = [id_param_pair, id_cluster, p1, p2, len(clusters_), \n",
    "                                             len(clus_), hetero_fraction_, syn_ids, tids]\n",
    "            id_cluster += 1\n",
    "        syn_ids = [syn.sid for syn in unclustered_]\n",
    "        tids = [sid_tid[syn.sid] for syn in unclustered_]\n",
    "        syn_stats.loc[len(syn_stats)] = [id_param_pair, 'unclustered', p1, p2, -1, \n",
    "                                         len(unclustered_), hetero_fraction_, syn_ids, tids]\n",
    "        id_param_pair += 1\n",
    "    num_clusters.append(n_cl_)\n",
    "    fractions.append(frac_)\n",
    "    \n",
    "syn_stats.to_csv('clust_stats_path.csv', index=False)\n",
    "save_obj(data_to_save, 'saved_param_clusters_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(z=num_clusters, y=[str(x) for x in params_1], x=[str(x) for x in params_2]),\n",
    "    layout=go.Layout(\n",
    "        xaxis=dict(title='param2', tickvals=params_2),\n",
    "        yaxis=dict(title='param1', tickvals=params_1)\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hetero Fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(z=fractions, y=[str(x) for x in params_1], x=[str(x) for x in params_2]),\n",
    "    layout=go.Layout(\n",
    "        xaxis=dict(title='param2', tickvals=params_2),\n",
    "        yaxis=dict(title='param1', tickvals=params_1)\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
